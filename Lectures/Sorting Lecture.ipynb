{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting\n",
    "\n",
    "In this lecture we will focus on algorithms to sort elements. The sorting algorithms we will cover in order of descending time complexity\n",
    "\n",
    "- Insertion Sort\n",
    "- Bubble Sort\n",
    "- Shellsort\n",
    "- Heapsort\n",
    "- Merge Sort \n",
    "- Quicksort\n",
    "- Bucket Sort\n",
    "- Radix Sort\n",
    "\n",
    "We will also cover external sorting for when an object is too large to fit into memory. \n",
    "\n",
    "## Insertion Sort\n",
    "One of the simpliest sorting algorithms is **insertion sort**. Insertion sort consist of N-1 **passes**. At each pass, p, we ensure that the array from 0 to p is in sorted order. Since we know that all the elements from 0 to p-1 are in sorted order we need to find out where element p belongs in the already sorted list. So in each pass p we move the element in position p left until it's in the correct position. Lets look at how this is implemented\n",
    "```python\n",
    "def insertion_sort(arr): \n",
    "    # iterate through the array\n",
    "    for i in range(1, len(arr)): \n",
    "  \n",
    "        key = arr[i] \n",
    "  \n",
    "        # Move elements of arr[0..i-1], that are \n",
    "        # greater than key, to one position ahead \n",
    "        # of their current position \n",
    "        j = i\n",
    "        while j > 0 and key < arr[j-1] : \n",
    "                arr[j] = arr[j-1] \n",
    "                j -= 1\n",
    "        arr[j] = key\n",
    "    return arr\n",
    "```\n",
    "Because of the nested loops, each of which can take N iterations, insertion sort runs in $O(N^2)$. We could easily see this is possible if the input is reversed. However if the list is already sorted then the inner while loop never runs hence giving it O(N) running time. \n",
    "Lets look at a visualization of the algorithm. \n",
    "![SegmentLocal](./files/Sorting/insertion_sort.gif \"segment\")\n",
    "\n",
    "## Bubble Sort\n",
    "Another extremely simple sorting algorithm is **bubble sort**. Bubble sort works by repeatedly stepping through the list and comparing each pair of adjacent elements and swapping them if they are in the wrong order. The algorithm continually steps through the list until no swaps are performed during a pass. During each pass bubble sort is finding the $n^{th}$ largest element and putting it into the correct place. Similar to insertion sort, the running time of this algorithm is $O(N^2)$. Hence if the list is in reversed order it will take N passes to find the smallest element in the list and put it in the correct position. Let's look at how bubble sort is implemented\n",
    "```python\n",
    "def bubble_sort(arr):\n",
    "    # There is also no do while in python\n",
    "    # so we will emulate one by initializing swapped to True\n",
    "    # and setting it to false at the beginning of the loop\n",
    "    swapped = True\n",
    "    while swapped:\n",
    "        swapped = False\n",
    "        # Perform a pass on the array \n",
    "        for i in range(len(arr)-1):\n",
    "            # If the elements are in the wrong order swap them\n",
    "            # set swapped to True as a swap has been performed\n",
    "            if arr[i] > arr[i+1]:\n",
    "                arr[i], arr[i+1] = arr[i+1], arr[i]\n",
    "                swapped = True\n",
    "    return arr\n",
    "```\n",
    "Lets look at a visualization of the algorithm in work.\n",
    "![SegmentLocal](./files/Sorting/bubble_sort.gif \"segment\")\n",
    "\n",
    "## Shell Sort\n",
    "Shell sort takes the idea of insertion sort and generalizes it. The idea is that to arrange the list of elements so that starting from anywhere every $h^{th}$ element from that position is sorted, or we can state that for every i, a\\[i\\] $\\le$ a\\[i+h\\]. The list at this point is said to be **h-sorted**. The algorithm starts by initially considering a large h and then decreases until the last phase in which h=1. The question is how to determine the sequence of h (gaps) to use when performing the sort. Let's look at an example if we chose 5,3,1 as the gaps.\n",
    "\n",
    "<img src=\"./files/Sorting/shellsort.png\" width=\"600\"/>\n",
    "\n",
    "\n",
    "Shell suggest $h_1 = N/2$, and $h_k = N/2^k$. However the problem with Shell's method is that is has a running time of $O(N^2)$ so there is no obvious case for using it over insertion or bubble sort. We can show that the upper bound using Shell's method is $O(N^2)$. Each pass with an increment of $h_k$ consists of $h_k$ insertion sorts of $N/h_k$ elements. Since insertion sort is quadratic, the total cost of a pass is $O(h_k(N/h_k)^2) = O(N^2/h_k)$. Summing over all the passes gives a total bound of $\\sum_{i=1}^{t} N^2/h_i = N^2\\sum_{i=1}^{t} 1/h_i$. Since the increments form a geometric series with a common ratio of 2, and the largest term in the series is 1, $\\sum_{i=1}^{t} 1/h_i < 2$. This we obtain a upper bound of $O(N^2)$.\n",
    "\n",
    "The problem with Shell's method is that the increments are not relatively prime, and thus smaller increments can have little effect. Hibbard sugggest a slightly different increment sequence, giving a better run time of $O(N^{3/2})$. Hibbard's method states to use increments of the form $1,3,7,...,2^k-1$. The key difference here is that each increment is prime therefore they have no common factors between the increments. The proof of this requires using additive number theory which is beyond the scope of this class and therefore left out. \n",
    "\n",
    "There are other methods that have been applied to shell sort not just Shell's and Hibbard's. Prat showed that using $O(N^{3/2})$ applies to a wide range of increment sequences. Whereas Sedgewick has proposed multiple increment sequences that give an $O(N^{4/3})$ worst case running time using $4^k+3 * 2^i + 1$. There are several more that use theorems from number theory and combinatorics such as Cuira's method which has yet to have a proven run time. Hence why the actual running time of shell sort is unknown.\n",
    "\n",
    "Let's look at code for using Hibbards algorithm\n",
    "```python\n",
    "def shell_sort(arr):\n",
    "    # Create the increment sequence\n",
    "    gaps = []\n",
    "    for i in range(1, int(log(len(arr),2))):\n",
    "        gaps.insert(0,(2 ** i - 1))\n",
    "    \n",
    "    for gap in gaps:\n",
    "        # perform generalized insertion sort for that gap\n",
    "        for i in range(gap, len(arr)):\n",
    "            key = arr[i]\n",
    "            j = i\n",
    "            while j >= gap and key < arr[j-gap]:\n",
    "                arr[j] = arr[j-gap]\n",
    "                j -= gap\n",
    "            \n",
    "            arr[j] = key\n",
    "        \n",
    "    return arr\n",
    "```\n",
    "\n",
    "## Heap Sort\n",
    "Binary Heaps (Priority Queues) can be used to sort in O(NlogN) time. So far this algorithm gives the best running time for sorting algorithms. Recall that the basic strategy of a min heap is that the minimum element is always at the root. So the idea is that given an unsorted list to build a binary heap of N elements (this takes O(N) time). We then perform N deleteMin operations getting the elements in sorted order. Each time we perform a deleteMin we save the result in a second array to hold the sorted elements. Since the running time of deleteMin is O(logN) and we perform it N times the total running time is O(NlogN). <br>\n",
    "The only problem with this algorithm is space complexity since it uses a second array to hold the sorted items. This is not usually a problem as memory is cheap and plentiful but could become an issue if the input is substantially large. A clever way to avoid using a second array is to use the array representing the binary heap to maintain the sorted order. Since after each delete the heap decreases size by 1 the cell that was last in the heap could be used to store the removal. If we were to use this strategy on a min heap it would return an array of items sorted in reverse order. To fix this instead of using a min heap we now use a max heap. Assuming we are using a max heap here is how we would implement a heap sort inside the heap class. We will use the same stucture as the binary heap class we implemented in the heaps lecture.\n",
    "```python\n",
    "def heap_sort(arr):\n",
    "    # first build a heap\n",
    "    self.build_heap(arr)\n",
    "    \n",
    "    # perform N delete maxes and place the max element in the spot that was just removed\n",
    "    # This sorts the array that the heap is built on\n",
    "    for i in range(len(self.heap)):\n",
    "        element = self.delete_max()\n",
    "        self.heap[self.size+1] = element\n",
    "    \n",
    "    # The array maintaining the heap is now sorted\n",
    "    # We remove the first element since the indexes in a heap start at 1\n",
    "    return self.heap[1:]\n",
    "```\n",
    "\n",
    "## Merge Sort \n",
    "The fundamental operation in mergesort is merging two sorted list. Because the lists are sorted, this can be done in one pass through the two already sorted list and the output a sorted list. This merging takes two sorted lists A and B, and output list C. The way we do this is pass through both list, A and B, simultaneously. We have counters for both A and B and while passing through the list we append the smaller of the two onto list C. We do this until we have fully passed through both list. Lets look at an example of how the merge operation works.\n",
    "\n",
    "Let array A contain \\[1, 13, 24, 26\\] and B contain \\[2, 15, 27, 38\\]. Let the counters for A and B be i and j respectively. \n",
    "<img src=\"./files/Sorting/mergesort_merge.png\" width=\"500\"/>\n",
    "\n",
    "We can see that once the counter has reached the end we just append the rest of the other list onto the output list since it is already sorted. The run time of this merge is O(N) since at most N-1 comparisons need to be made, where N is the total number of elements in the two lists. \n",
    "\n",
    "The algorithm is therefore easy to describe. If N = 1, there is only element to sort. Otherwise, recursively mergsort the first half and the second half. This gives two sorted halves which can be merged together using the merging algorithm. This algorithm is a classic divide-and-conquer strategy. The problem is *divided* into smaller problems and solved recursively. The *conquering* phase consists of patching together the answers. Lets look at the example:\n",
    "<img src=\"./files/Sorting/margesort.png\" width=\"500\"/>\n",
    "\n",
    "Lets look at the code\n",
    "```python\n",
    "# First the merge of two sorted arrays\n",
    "def merge(left, right):\n",
    "    # counters for both lists\n",
    "    counter_left, counter_right = 0\n",
    "    \n",
    "    # length of the smaller of the two list\n",
    "    min_length = min(len(left, right))\n",
    "    \n",
    "    # output list to hold sorted merge\n",
    "    sorted_output = []\n",
    "    \n",
    "    # loop through both lists simultaneously\n",
    "    while counter_left < min_length and counter_right < min_length:\n",
    "        if left[counter_left] < right[counter_right]:\n",
    "            sorted_output.append(left[counter_left])\n",
    "            counter_left += 1\n",
    "        else:\n",
    "            sorted_output.append(right[counter_right])\n",
    "            counter_right += 1\n",
    "            \n",
    "    # After exhausting one list append the rest of the list to the output\n",
    "    if counter_right < len(right):\n",
    "        sorted_output.extend(right[counter_right:])\n",
    "    elif counter_left < len(left):\n",
    "        sorted_output.extend(left[counter_left:])\n",
    "        \n",
    "    return sorted_output\n",
    "\n",
    "\n",
    "# Merge sort\n",
    "def merge_sort(arr):\n",
    "    if len(arr) == 1:\n",
    "        return arr\n",
    "    else:\n",
    "        center = len(arr) // 2\n",
    "        # split the arrray in half\n",
    "        left = merge_sort(arr[:center])\n",
    "        right = merge_sort(arr[center:])\n",
    "        # return the merge of the two sorted list\n",
    "        return merge(left, right)\n",
    "```\n",
    "\n",
    "### Running Time Analysis\n",
    "The runtime of mergesort is O(NlogN) but how do we know this? Lets prove it with recurrence relations. For N = 1, the time to mergesort is constant since a single element is already sorted, we will denote this by 1. Otherwise, the time to mergesort N numbers is equal to the time to do two recursive mergesorts of size N/2, plus the time to merge (which is linear). The following equations:\n",
    "<center> T(1) = 1 <br> T(N) = 2T(N/2)+N </center>\n",
    "Lets solve the reccurrence relations as follows:\n",
    "<center>\n",
    "    T(N) = 2T(N/2)+N <br>\n",
    "         = 2(2T(N/4) + N/2) + N = 4T(N/4) + 2N <br>\n",
    "         = 4(2T(N/8) + N/4) + 2N = 8T(N/8) + 3N <br>\n",
    "                           ...<br>\n",
    "         = $2^kT(N/2^k) + kN$ \n",
    "</center>\n",
    "Using k = logN, we obtain T(N) = NT(1) + NlogN = NlogN + N which gives us the running time of O(NlogN).\n",
    "\n",
    "## Quicksort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
